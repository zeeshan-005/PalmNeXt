# -*- coding: utf-8 -*-
"""01-cnn-attention-on-palm-leaves-dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kexv0XlHl-WV9YC7Xt-uEsyaBiB7dUOe
"""

import os
import cv2
import albumentations as A
import albumentations.pytorch
import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn, optim
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Subset
import pickle
import torch.nn.functional as F
from sklearn.metrics import confusion_matrix, accuracy_score

# Define transforms for data augmentation and normalization
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images to 224x224
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the dataset
root_dir = '/kaggle/input/palm-leaves-dataset/Palm Leaves Dataset'  # Path to the dataset root folder
dataset = datasets.ImageFolder(root=root_dir, transform=transform)

# Split dataset indices into training (75%), validation (15%), and test (10%)
train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.10, stratify=dataset.targets)
train_idx, val_idx = train_test_split(train_idx, test_size=0.15/0.90, stratify=[dataset.targets[i] for i in train_idx])

# Create subsets
train_dataset = Subset(dataset, train_idx)
val_dataset = Subset(dataset, val_idx)
test_dataset = Subset(dataset, test_idx)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Example to check the number of samples in each split
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Test samples: {len(test_dataset)}")

import torch

# Check if CUDA (GPU) is available, otherwise use the CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# Define the Attention Module
class SelfAttention(nn.Module):
    def __init__(self, in_dim):
        super(SelfAttention, self).__init__()
        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, C, width, height = x.size()  # Ensure the input has 4 dimensions
        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(batch_size, -1, width * height)
        energy = torch.bmm(proj_query, proj_key)
        attention = F.softmax(energy, dim=-1)
        proj_value = self.value_conv(x).view(batch_size, -1, width * height)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, width, height)
        out = self.gamma * out + x
        return out

# Define the complete model with AdaptiveAvgPool2d
class CNN_Attention(nn.Module):
    def __init__(self, num_classes=4):
        super(CNN_Attention, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # Output: (32, H, W)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output: (64, H, W)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # Output: (128, H, W)
        self.attention = SelfAttention(128)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Output: (128, 1, 1)
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x):
        # x shape: (batch_size, 3, H, W)
        x = F.relu(self.conv1(x))
        # x shape: (batch_size, 32, H, W)
        x = F.max_pool2d(x, 2)
        # x shape: (batch_size, 32, H/2, W/2)
        x = F.relu(self.conv2(x))
        # x shape: (batch_size, 64, H/2, W/2)
        x = F.max_pool2d(x, 2)
        # x shape: (batch_size, 64, H/4, W/4)
        x = F.relu(self.conv3(x))
        # x shape: (batch_size, 128, H/4, W/4)
        x = self.attention(x)
        # x shape: (batch_size, 128, H/4, W/4)
        x = F.max_pool2d(x, 2)
        # x shape: (batch_size, 128, H/8, W/8)
        x = self.avgpool(x)
        # x shape: (batch_size, 128, 1, 1)
        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 128)
        x = self.fc(x)
        return F.log_softmax(x, dim=1)



# Instantiate the model, loss function, and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNN_Attention(num_classes=4).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training function
def train(model, train_loader, criterion, optimizer):
    model.train()
    train_loss = 0
    correct = 0
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()

    train_loss /= len(train_loader.dataset)
    train_accuracy = 100. * correct / len(train_loader.dataset)
    return train_loss, train_accuracy

# Validation function
def validate(model, val_loader, criterion):
    model.eval()
    val_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            val_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    val_loss /= len(val_loader.dataset)
    val_accuracy = 100. * correct / len(val_loader.dataset)
    return val_loss, val_accuracy

num_epochs = 20
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []

for epoch in range(1, num_epochs + 1):
    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer)
    val_loss, val_accuracy = validate(model, val_loader, criterion)

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_accuracy)
    val_accuracies.append(val_accuracy)

    print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')

# Save the trained model and history
torch.save(model.state_dict(), 'cnn_attention_model.pth')
history = {'train_losses': train_losses, 'val_losses': val_losses, 'train_accuracies': train_accuracies, 'val_accuracies': val_accuracies}
with open('training_history.pkl', 'wb') as f:
    pickle.dump(history, f)

# Load training history
with open('training_history.pkl', 'rb') as f:
    history = pickle.load(f)

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history['train_losses'], label='Train Loss')
plt.plot(history['val_losses'], label='Validation Loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history['train_accuracies'], label='Train Accuracy')
plt.plot(history['val_accuracies'], label='Validation Accuracy')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.show()

def test(model, test_loader, device):
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for data, target in test_loader:
            # Move data and target to the GPU
            data, target = data.to(device), target.to(device)

            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            all_preds.extend(pred.cpu().numpy())  # Move predictions to CPU for further processing
            all_targets.extend(target.cpu().numpy())  # Move targets to CPU for further processing

    return all_preds, all_targets

# Assuming val_loader is being used as the test_loader here
test_preds, test_targets = test(model, val_loader, device)

# Compute confusion matrix and accuracy
conf_matrix = confusion_matrix(test_targets, test_preds)
test_accuracy = accuracy_score(test_targets, test_preds)

print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
plt.figure(figsize=(8, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report
print(classification_report(test_targets, test_preds))

